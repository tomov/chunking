% -----------------------------------------------------------------------
% File: CogSci_Template.bib
% -----------------------------------------------------------------------

% Modified : Eli M. Silk (esilk at pitt.edu)            05/24/2005
% Modified : David Noelle (dnoelle at ucmerced.edu)     11/19/2014


@article{Botvinick2008,
	author = {Botvinick, Matthew and Niv, Yael and C Barto, Andrew},
	year = {2008},
	month = {11},
	pages = {262-80},
	title = {Hierarchically organized behavior and its neural foundations: A reinforcement learning perspective},
	volume = {113},
	journal = {Cognition}
}

@article{Smith2013,
	author = {Smith, Kyle and Graybiel, Ann},
	year = {2013},
	month = {01},
	pages = {},
	title = {Using Optogenetics to Study Habits},
	volume = {1511},
	journal = {Brain Research}
}


@Article{Dolan2013,
	author={Dolan, Ray J and Dayan, Peter},
	title={Goals and Habits in the Brain},
	journal={Neuron},
	year={2013},
	publisher={Elsevier},
	volume={80},
	number={2},
	pages={312-325},
	abstract={An enduring and richly elaborated dichotomy in cognitive neuroscience is that of reflective versus reflexive decision making and choice. Other literatures refer to the two ends of what is likely to be a spectrum with terms such as goal-directed versus habitual, model-based versus model-free or prospective versus retrospective. One of the most rigorous traditions of experimental work in the field started with studies in rodents and graduated via human versions and enrichments of those experiments to a current state in which new paradigms are probing and challenging the very heart of the distinction. We review four generations of work in this tradition and provide pointers to the forefront of the field?s fifth generation.},
	issn={0896-6273},
	doi={10.1016/j.neuron.2013.09.007},
	url={http://dx.doi.org/10.1016/j.neuron.2013.09.007}
}

@article{Wilson2007,
	title = "Multi-task reinforcement learning: A hierarchical Bayesian approach",
	abstract = "We consider the problem of multi-task reinforcement learning, where the agent needs to solve a sequence of Markov Decision Processes (MDPs) chosen randomly from a fixed but unknown distribution. We model the distribution over MDPs using a hierarchical Bayesian infinite mixture model. For each novel MDP, we use the previously learned distribution as an informed prior for modelbased Bayesian reinforcement learning. The hierarchical Bayesian framework provides a strong prior that allows us to rapidly infer the characteristics of new environments based on previous environments, while the use of a nonparametric model allows us to quickly adapt to environments we have not encountered before. In addition, the use of infinite mixtures allows for the model to automatically learn the number of underlying MDP components. We evaluate our approach and show that it leads to significant speedups in convergence to an optimal policy after observing only a small number of tasks.",
	author = "Aaron Wilson and Alan Fern and Soumya Ray and Prasad Tadepalli",
	year = "2007",
	doi = "10.1145/1273496.1273624",
	volume = "227",
	pages = "1015--1022",
	journal = "ACM International Conference Proceeding Series",
}

@article{Botvinick2015,
	title = "Reinforcement learning, efficient coding, and the statistics of natural tasks ",
	journal = "Current Opinion in Behavioral Sciences ",
	volume = "5",
	number = "",
	pages = "71 - 77",
	year = "2015",
	note = "Neuroeconomics ",
	issn = "2352-1546",
	doi = "https://doi.org/10.1016/j.cobeha.2015.08.009",
	url = "https://www.sciencedirect.com/science/article/pii/S2352154615001151",
	author = "Matthew Botvinick and Ari Weinstein and Alec Solway and Andrew Barto",
	abstract = "The application of ideas from computational reinforcement learning has recently enabled dramatic advances in behavioral and neuroscientific research. For the most part, these advances have involved insights concerning the algorithms underlying learning and decision making. In the present article, we call attention to the equally important but relatively neglected question of how problems in learning and decision making are internally represented. To articulate the significance of representation for reinforcement learning we draw on the concept of efficient coding, as developed in perception research. The resulting perspective exposes a range of novel goals for behavioral and neuroscientific research, highlighting in particular the need for research into the statistical structure of naturalistic tasks. "
}


@incollection{Shi2009,
	title = {Neural Implementation of Hierarchical Bayesian Inference by Importance Sampling},
	author = {Shi, Lei and Thomas L. Griffiths},
	booktitle = {Advances in Neural Information Processing Systems 22},
	editor = {Y. Bengio and D. Schuurmans and J. D. Lafferty and C. K. I. Williams and A. Culotta},
	pages = {1669--1677},
	year = {2009},
	publisher = {Curran Associates, Inc.},
	url = {http://papers.nips.cc/paper/3782-neural-implementation-of-hierarchical-bayesian-inference-by-importance-sampling.pdf}
}


@article{Li2009,
 author = {Li, Hui and Liao, Xuejun and Carin, Lawrence},
 title = {Multi-task Reinforcement Learning in Partially Observable Stochastic Environments},
 journal = {Journal of Machine Learning Research},
 issue_date = {12/1/2009},
 volume = {10},
 month = jun,
 year = {2009},
 issn = {1532-4435},
 pages = {1131--1186},
 numpages = {56},
 url = {http://dl.acm.org/citation.cfm?id=1577069.1577109},
 acmid = {1577109},
 publisher = {JMLR.org},
} 

@article{Dezfouli2013,
    author = {Dezfouli, Amir AND Balleine, Bernard W.},
    journal = {PLOS Computational Biology},
    publisher = {Public Library of Science},
    title = {Actions, Action Sequences and Habits: Evidence That Goal-Directed and Habitual Action Control Are Hierarchically Organized},
    year = {2013},
    month = {12},
    volume = {9},
    url = {https://doi.org/10.1371/journal.pcbi.1003364},
    pages = {1-14},
    abstract = {Author Summary In order to make choices that lead to desirable outcomes, individuals tend to deliberate over the consequences of various alternatives. This goal-directed deliberation is, however, slow and cognitively demanding. As a consequence, under appropriate conditions decision-making can become habitual and automatic. The nature of these habitual actions, how they are learned, expressed, and interact with the goal-directed process is not clearly understood. Here we report that (1) habits interact with the goal-directed process in a hierarchical manner (i.e., the goal-directed system selects a goal, and then determines which habit should be executed to reach that goal), and (2) habits are learned sequences of actions that, once triggered by the goal-directed process, can be expressed quickly and in an efficient manner. The findings provide critical new experimental and computational information on the nature of habits and how they interact with the goal-directed decision-making.},
    number = {12},
    doi = {10.1371/journal.pcbi.1003364}
}

@article{Sutton1999,
	title = "Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning",
	journal = "Artificial Intelligence",
	volume = "112",
	number = "1",
	pages = "181 - 211",
	year = "1999",
	issn = "0004-3702",
	doi = "https://doi.org/10.1016/S0004-3702(99)00052-1",
	url = "http://www.sciencedirect.com/science/article/pii/S0004370299000521",
	author = "Richard S. Sutton and Doina Precup and Satinder Singh",
	keywords = "Temporal abstraction, Reinforcement learning, Markov decision processes, Options, Macros, Macroactions, Subgoals, Intra-option learning, Hierarchical planning, Semi-Markov decision processes",
	abstract = "Abstract Learning, planning, and representing knowledge at multiple levels of temporal abstraction are key, longstanding challenges for AI. In this paper we consider how these challenges can be addressed within the mathematical framework of reinforcement learning and Markov decision processes (MDPs). We extend the usual notion of action in this framework to include optionsâclosed-loop policies for taking action over a period of time. Examples of options include picking up an object, going to lunch, and traveling to a distant city, as well as primitive actions such as muscle twitches and joint torques. Overall, we show that options enable temporally abstract knowledge and action to be included in the reinforcement learning framework in a natural and general way. In particular, we show that options may be used interchangeably with primitive actions in planning methods such as dynamic programming and in learning methods such as Q-learning. Formally, a set of options defined over an MDP constitutes a semi-Markov decision process (SMDP), and the theory of SMDPs provides the foundation for the theory of options. However, the most interesting issues concern the interplay between the underlying MDP and the SMDP and are thus beyond SMDP theory. We present results for three such cases: (1) we show that the results of planning with options can be used during execution to interrupt options and thereby perform even better than planned, (2) we introduce new intra-option methods that are able to learn about an option from fragments of its execution, and (3) we propose a notion of subgoal that can be used to improve the options themselves. All of these results have precursors in the existing literature; the contribution of this paper is to establish them in a simpler and more general setting with fewer changes to the existing reinforcement learning framework. In particular, we show that these results can be obtained without committing to (or ruling out) any particular approach to state abstraction, hierarchy, function approximation, or the macro-utility problem."
}

@article{Machado2017,
  author    = {Marlos C. Machado and
               Marc G. Bellemare and
               Michael H. Bowling},
  title     = {A Laplacian Framework for Option Discovery in Reinforcement Learning},
  journal   = {Computing Research Repository},
  volume    = {abs/1703.00956},
  year      = {2017},
  url       = {http://arxiv.org/abs/1703.00956},
  archivePrefix = {arXiv},
  eprint    = {1703.00956},
  timestamp = {Wed, 07 Jun 2017 14:40:23 +0200},
  biburl    = {http://dblp.org/rec/bib/journals/corr/MachadoBB17},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@Article{Friston2010,
	author={Friston, Karl},
	title={The free-energy principle: a unified brain theory?},
	journal={Nature Reviews Neuroscience},
	year={2010},
	month={Feb},
	publisher={Nature Publishing Group},
	volume={11},
	number={2},
	pages={127-138},
	issn={1471-003X},
	doi={10.1038/nrn2787},
	url={http://dx.doi.org/10.1038/nrn2787}
}

@article{Solway2014,
    author = {Solway, Alec AND Diuk, Carlos AND C{\'o}rdova, Natalia AND Yee, Debbie AND Barto, Andrew G. AND Niv, Yael AND Botvinick, Matthew M.},
    journal = {PLOS Computational Biology},
    publisher = {Public Library of Science},
    title = {Optimal Behavioral Hierarchy},
    year = {2014},
    month = {08},
    volume = {10},
    url = {https://doi.org/10.1371/journal.pcbi.1003779},
    pages = {1-10},
    abstract = {Author Summary In order to accomplish everyday tasks, we often divide them up into subtasks: to make spaghetti, we (1) get out a pot, (2) fill it with water, (3) bring the water to a boil, and so forth. But how do we learn to subdivide our goals in this way? Work from computer science suggests that the way a task is subdivided or decomposed can have a dramatic impact on how easy the task is to accomplish: certain decompositions speed learning and planning compared to others. Moreover, some decompositions allow behaviors to be represented more simply. Despite this general insight, little work has been done to formalize these ideas. We outline a mathematical framework to address this question, based on methods for comparing between statistical models. We then present four behavioral experiments, showing that human learners spontaneously discover optimal task decompositions.},
    number = {8},
    doi = {10.1371/journal.pcbi.1003779}
}

@Article{Ribas-Fernandes2011,
author={Ribas-Fernandes, Jos{\'e} F.
and Solway, Alec
and Diuk, Carlos
and McGuire, Joseph T
and Barto, Andrew G
and Niv, Yael
and Botvinick, Matthew M},
title={A Neural Signature of Hierarchical Reinforcement Learning},
journal={Neuron},
year={2011},
month={2017/10/26},
publisher={Elsevier},
volume={71},
number={2},
pages={370-379},
abstract={Human behavior displays hierarchical structure: simple actions cohere into subtask sequences, which work together to accomplish overall task goals. Although the neural substrates of such hierarchy have been the target of increasing research, they remain poorly understood. We propose that the computations supporting hierarchical behavior may relate to those in hierarchical reinforcement learning (HRL), a machine-learning framework that extends reinforcement-learning mechanisms into hierarchical domains. To test this, we leveraged a distinctive prediction arising from HRL. In ordinary reinforcement learning, reward prediction errors are computed when there is an unanticipated change in the prospects for accomplishing overall task goals. HRL entails that prediction errors should also occur in relation to task subgoals. In three neuroimaging studies we observed neural responses consistent with such subgoal-related reward prediction errors, within structures previously implicated in reinforcement learning. The results reported support the relevance of HRL to the neural processes underlying hierarchical behavior.},
issn={0896-6273},
doi={10.1016/j.neuron.2011.05.042},
url={http://dx.doi.org/10.1016/j.neuron.2011.05.042}
}


@article {Botvinick2014,
    author = {Botvinick, Matthew and Weinstein, Ari},
    title = {Model-based hierarchical reinforcement learning and human action control},
    volume = {369},
    number = {1655},
    year = {2014},
    doi = {10.1098/rstb.2013.0480},
    publisher = {The Royal Society},
    abstract = {Recent work has reawakened interest in goal-directed or {\textquoteleft}model-based{\textquoteright} choice, where decisions are based on prospective evaluation of potential action outcomes. Concurrently, there has been growing attention to the role of hierarchy in decision-making and action control. We focus here on the intersection between these two areas of interest, considering the topic of hierarchical model-based control. To characterize this form of action control, we draw on the computational framework of hierarchical reinforcement learning, using this to interpret recent empirical findings. The resulting picture reveals how hierarchical model-based mechanisms might play a special and pivotal role in human decision-making, dramatically extending the scope and complexity of human behaviour.},
    issn = {0962-8436},
    URL = {http://rstb.royalsocietypublishing.org/content/369/1655/20130480},
    eprint = {http://rstb.royalsocietypublishing.org/content/369/1655/20130480.full.pdf},
    journal = {Philosophical Transactions of the Royal Society B: Biological Sciences}
}

@Article{Girshick2011,
	author={Girshick, Ahna R.
	and Landy, Michael S.
	and Simoncelli, Eero P.},
	title={Cardinal rules: visual orientation perception reflects knowledge of environmental statistics},
	journal={Nat Neurosci},
	year={2011},
	month={Jul},
	publisher={Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
	volume={14},
	number={7},
	pages={926-932},
	issn={1097-6256},
	doi={10.1038/nn.2831},
	url={http://dx.doi.org/10.1038/nn.2831}
}

@techreport{Marr1976,
 author = {Marr, David and Poggio, Tomaso},
 title = {From Understanding Computation to Understanding Neural Circuitry},
 year = {1976},
 source = {http://www.ncstrl.org:8900/ncstrl/servlet/search?formname=detail\&id=oai%3Ancstrlh%3Amitai%3AMIT-AILab%2F%2FAIM-357},
 publisher = {Massachusetts Institute of Technology},
 address = {Cambridge, MA, USA},
} 


@article {Friston2005,
    author = {Friston, Karl},
    title = {A theory of cortical responses},
    volume = {360},
    number = {1456},
    pages = {815--836},
    year = {2005},
    doi = {10.1098/rstb.2005.1622},
    publisher = {The Royal Society},
    abstract = {This article concerns the nature of evoked brain responses and the principles underlying their generation. We start with the premise that the sensory brain has evolved to represent or infer the causes of changes in its sensory inputs. The problem of inference is well formulated in statistical terms. The statistical fundaments of inference may therefore afford important constraints on neuronal implementation. By formulating the original ideas of Helmholtz on perception, in terms of modern-day statistical theories, one arrives at a model of perceptual inference and learning that can explain a remarkable range of neurobiological facts.It turns out that the problems of inferring the causes of sensory input (perceptual inference) and learning the relationship between input and cause (perceptual learning) can be resolved using exactly the same principle. Specifically, both inference and learning rest on minimizing the brain{\textquoteright}s free energy, as defined in statistical physics. Furthermore, inference and learning can proceed in a biologically plausible fashion. Cortical responses can be seen as the brain{\textquoteright}s attempt to minimize the free energy induced by a stimulus and thereby encode the most likely cause of that stimulus. Similarly, learning emerges from changes in synaptic efficacy that minimize the free energy, averaged over all stimuli encountered. The underlying scheme rests on empirical Bayes and hierarchical models of how sensory input is caused. The use of hierarchical models enables the brain to construct prior expectations in a dynamic and context-sensitive fashion. This scheme provides a principled way to understand many aspects of cortical organization and responses. The aim of this article is to encompass many apparently unrelated anatomical, physiological and psychophysical attributes of the brain within a single theoretical perspective.In terms of cortical architectures, the theoretical treatment predicts that sensory cortex should be arranged hierarchically, that connections should be reciprocal and that forward and backward connections should show a functional asymmetry (forward connections are driving, whereas backward connections are both driving and modulatory). In terms of synaptic physiology, it predicts associative plasticity and, for dynamic models, spike-timing-dependent plasticity. In terms of electrophysiology, it accounts for classical and extra classical receptive field effects and long-latency or endogenous components of evoked cortical responses. It predicts the attenuation of responses encoding prediction error with perceptual learning and explains many phenomena such as repetition suppression, mismatch negativity (MMN) and the P300 in electroencephalography. In psychophysical terms, it accounts for the behavioural correlates of these physiological phenomena, for example, priming and global precedence. The final focus of this article is on perceptual learning as measured with the MMN and the implications for empirical studies of coupling among cortical areas using evoked sensory responses.},
    issn = {0962-8436},
    URL = {http://rstb.royalsocietypublishing.org/content/360/1456/815},
    eprint = {http://rstb.royalsocietypublishing.org/content/360/1456/815.full.pdf},
    journal = {Philosophical Transactions of the Royal Society B: Biological Sciences}
}

@Article{Starkweather2017,
	author={Starkweather, Clara Kwon
	and Babayan, Benedicte M.
	and Uchida, Naoshige
	and Gershman, Samuel J.},
	title={Dopamine reward prediction errors reflect hidden-state inference across time},
	journal={Nat Neurosci},
	year={2017},
	month={Apr},
	publisher={Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
	volume={20},
	number={4},
	pages={581-589},
	abstract={Midbrain dopamine neurons signal reward prediction error (RPE), or actual minus expected reward. The temporal difference (TD) learning model has been a cornerstone in understanding how dopamine RPEs could drive associative learning. Classically, TD learning imparts value to features that serially track elapsed time relative to observable stimuli. In the real world, however, sensory stimuli provide ambiguous information about the hidden state of the environment, leading to the proposal that TD learning might instead compute a value signal based on an inferred distribution of hidden states (a 'belief state'). Here we asked whether dopaminergic signaling supports a TD learning framework that operates over hidden states. We found that dopamine signaling showed a notable difference between two tasks that differed only with respect to whether reward was delivered in a deterministic manner. Our results favor an associative learning rule that combines cached values with hidden-state inference.},
	note={Article},
	issn={1097-6256},
	url={http://dx.doi.org/10.1038/nn.4520}
}


@book{James1899,
  TITLE = {Talks to Teashers on Psychology},
  SUBTITLE = {And to Students on Some of Life's Ideals},
  AUTHOR = {James, William},
  YEAR = {1899},
  PUBLISHER = {Henry Holtand Company, New York},
}

@article{Taylor2009,
 author = {Taylor, Matthew E. and Stone, Peter},
 title = {Transfer Learning for Reinforcement Learning Domains: A Survey},
 journal = {J. Mach. Learn. Res.},
 issue_date = {12/1/2009},
 volume = {10},
 month = dec,
 year = {2009},
 issn = {1532-4435},
 pages = {1633--1685},
 numpages = {53},
 url = {http://dl.acm.org/citation.cfm?id=1577069.1755839},
 acmid = {1755839},
 publisher = {JMLR.org},
} 

@article{Bilalic2008,
title = "Inflexibility of experts -- Reality or myth? Quantifying the Einstellung effect in chess masters",
journal = "Cognitive Psychology",
volume = "56",
number = "2",
pages = "73 - 102",
year = "2008",
issn = "0010-0285",
doi = "https://doi.org/10.1016/j.cogpsych.2007.02.001",
url = "http://www.sciencedirect.com/science/article/pii/S0010028507000102",
author = "Merim Bilali{\'c} and Peter McLeod and Fernand Gobet",
keywords = "Flexibility, Expertise, Einstellung (set) effect, Fixation, Skill acquisition, Automatization, Creativity, Education, Chess",
abstract = "Abstract How does the knowledge of experts affect their behaviour in situations that require unusual methods of dealing? One possibility, loosely originating in research on creativity and skill acquisition, is that an increase in expertise can lead to inflexibility of thought due to automation of procedures. Yet another possibility, based on expertise research, is that expertsâ knowledge leads to flexibility of thought. We tested these two possibilities in a series of experiments using the Einstellung (set) effect paradigm. Chess players tried to solve problems that had both a familiar but non-optimal solution and a better but less familiar one. The more familiar solution induced the Einstellung (set) effect even in experts, preventing them from finding the optimal solution. The presence of the non-optimal solution reduced expertsâ problem solving ability was reduced to about that of players three standard deviations lower in skill level by the presence of the non-optimal solution. Inflexibility of thought induced by prior knowledge (i.e., the blocking effect of the familiar solution) was shown by experts but the more expert they were, the less prone they were to the effect. Inflexibility of experts is both reality and myth. But the greater the level of expertise, the more of a myth it becomes."
}

@article {Tenenbaum2011,
    author = {Tenenbaum, Joshua B. and Kemp, Charles and Griffiths, Thomas L. and Goodman, Noah D.},
    title = {How to Grow a Mind: Statistics, Structure, and Abstraction},
    volume = {331},
    number = {6022},
    pages = {1279--1285},
    year = {2011},
    doi = {10.1126/science.1192788},
    publisher = {American Association for the Advancement of Science},
    abstract = {In coming to understand the world{\textemdash}in learning concepts, acquiring language, and grasping causal relations{\textemdash}our minds make inferences that appear to go far beyond the data available. How do we do it? This review describes recent approaches to reverse-engineering human learning and cognitive development and, in parallel, engineering more humanlike machine learning systems. Computational models that perform probabilistic inference over hierarchies of flexibly structured representations can address some of the deepest questions about the nature and origins of human thought: How does abstract knowledge guide learning and reasoning from sparse data? What forms does our knowledge take, across different domains and tasks? And how is that abstract knowledge itself acquired?},
    issn = {0036-8075},
    URL = {http://science.sciencemag.org/content/331/6022/1279},
    eprint = {http://science.sciencemag.org/content/331/6022/1279.full.pdf},
    journal = {Science}
}

@article{Knill2004,
	title = "The Bayesian brain: the role of uncertainty in neural coding and computation",
	journal = "Trends in Neurosciences",
	volume = "27",
	number = "12",
	pages = "712 - 719",
	year = "2004",
	issn = "0166-2236",
	doi = "https://doi.org/10.1016/j.tins.2004.10.007",
	url = "http://www.sciencedirect.com/science/article/pii/S0166223604003352",
	author = "David C. Knill and Alexandre Pouget",
	abstract = "To use sensory information efficiently to make judgments and guide action in the world, the brain must represent and use information about uncertainty in its computations for perception and action. Bayesian methods have proven successful in building computational theories for perception and sensorimotor control, and psychophysics is providing a growing body of evidence that human perceptual computations are âBayes' optimalâ. This leads to the âBayesian coding hypothesisâ: that the brain represents sensory information probabilistically, in the form of probability distributions. Several computational schemes have recently been proposed for how this might be achieved in populations of neurons. Neurophysiological data on the hypothesis, however, is almost non-existent. A major challenge for neuroscientists is to test these ideas experimentally, and so determine whether and how neurons code information about sensory uncertainty."
}

@article {Shepard1987,
    author = {Shepard, RN},
    title = {Toward a universal law of generalization for psychological science},
    volume = {237},
    number = {4820},
    pages = {1317--1323},
    year = {1987},
    doi = {10.1126/science.3629243},
    publisher = {American Association for the Advancement of Science},
    abstract = {A psychological space is established for any set of stimuli by determining metric distances between the stimuli such that the probability that a response learned to any stimulus will generalize to any other is an invariant monotonic function of the distance between them. To a good approximation, this probability of generalization (i) decays exponentially with this distance, and (ii) does so in accordance with one of two metrics, depending on the relation between the dimensions along which the stimuli vary. These empirical regularities are mathematically derivable from universal principles of natural kinds and probabilistic geometry that may, through evolutionary internalization, tend to govern the behaviors of all sentient organisms.},
    issn = {0036-8075},
    URL = {http://science.sciencemag.org/content/237/4820/1317},
    eprint = {http://science.sciencemag.org/content/237/4820/1317.full.pdf},
    journal = {Science}
}

@inbook {Chater1998,
	title = {Rational Models of Cognition},
	author = {Chater, Nick and Oaksford, Mike},
	publisher = {John Wiley \& Sons, Ltd},
	isbn = {9780470018866},
	url = {http://dx.doi.org/10.1002/0470018860.s00603},
	doi = {10.1002/0470018860.s00603},
	keywords = {rational analysis, rationality, optimality, Bayesian models, categorization, belief revision, Bayesian networks},
	booktitle = {Encyclopedia of Cognitive Science},
	year = {2006},
	abstract = {Rational models of cognition attempt to explain the function or purpose of cognitive processes.},
}

@article{Griffiths2001,
	author = {B. Griffiths, Joshua and L. Generalization, Thomas},
	year = {2000},
	month = {10},
	pages = {},
	title = {Generalization, Similarity, and Bayesian Inference},
	volume = {24},
	booktitle = {The Behavioral and Brain Sciences}
}

@book{Anderson1990,
  TITLE = {The Adaptive Character of Thought},
  SUBTITLE = {Studies in Cognition},
  AUTHOR = {R Anderson, John},
  YEAR = {1990},
  PUBLISHER = {Hillsdale, NJ: Erlbaum.},
}

@inbook {Helmholtz1866,
	title = {Concerning the perceptions in general, 3rd edn},
	author = {von Helmholtz, Hermann},
	booktitle = {Treatise on Physiological Optics, Vol. III},
    PUBLISHER = {(translated by J. P. C. Southall 1925 Opt. Soc. Am. Section 26, reprinted New York: Dover, 1962)},
	year = {1866},
}

@book{Sutton1998,
 author = {Sutton, Richard S. and Barto, Andrew G.},
 title = {Introduction to Reinforcement Learning},
 year = {1998},
 isbn = {0262193981},
 edition = {1st},
 publisher = {MIT Press},
 address = {Cambridge, MA, USA},
} 

@Article{Schapiro2013,
    author={Schapiro, Anna C.
        and Rogers, Timothy T.
            and Cordova, Natalia I.
            and Turk-Browne, Nicholas B.
            and Botvinick, Matthew M.},
    title={Neural representations of events arise from temporal community structure},
    journal={Nature Neuroscience},
    year={2013},
    month={Apr},
    publisher={Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
    volume={16},
    number={4},
    pages={486-492},
    issn={1097-6256},
    doi={10.1038/nn.3331},
    url={http://dx.doi.org/10.1038/nn.3331}
}

@article{tran2016edward,
  author = {Dustin Tran and Alp Kucukelbir and Adji B. Dieng and Maja Rudolph and Dawen Liang and David M. Blei},
  title = {{Edward: A library for probabilistic modeling, inference, and criticism}},
  journal = {arXiv preprint arXiv:1610.09787},
  year = {2016}
}

@article{Kemp2008,
	author = {Kemp, Charles and Tenenbaum, Joshua B.}, 
	title = {The discovery of structural form},
	volume = {105}, 
	number = {31}, 
	pages = {10687-10692}, 
	year = {2008}, 
	doi = {10.1073/pnas.0802631105}, 
	abstract ={Algorithms for finding structure in data have become increasingly important both as tools for scientific data analysis and as models of human learning, yet they suffer from a critical limitation. Scientists discover qualitatively new forms of structure in observed data: For instance, Linnaeus recognized the hierarchical organization of biological species, and Mendeleev recognized the periodic structure of the chemical elements. Analogous insights play a pivotal role in cognitive development: Children discover that object category labels can be organized into hierarchies, friendship networks are organized into cliques, and comparative relations (e.g., “bigger than” or “better than”) respect a transitive order. Standard algorithms, however, can only learn structures of a single form that must be specified in advance: For instance, algorithms for hierarchical clustering create tree structures, whereas algorithms for dimensionality-reduction create low-dimensional spaces. Here, we present a computational model that learns structures of many different forms and that discovers which form is best for a given dataset. The model makes probabilistic inferences over a space of graph grammars representing trees, linear orders, multidimensional spaces, rings, dominance hierarchies, cliques, and other forms and successfully discovers the underlying structure of a variety of physical, biological, and social domains. Our approach brings structure learning methods closer to human abilities and may lead to a deeper computational understanding of cognitive development.}, 
	URL = {http://www.pnas.org/content/105/31/10687.abstract}, 
	eprint = {http://www.pnas.org/content/105/31/10687.full.pdf}, 
	journal = {Proceedings of the National Academy of Sciences} 
}

@article{Goodman2011,
	author = {D Goodman, Noah and Ullman, Tomer and B Tenenbaum, Joshua},
	year = {2011},
	month = {04},
	pages = {110-9},
	title = {Learning a Theory of Causality},
	volume = {118},
	journal = {Psychological review}
}

@incollection{Gershman2009,
title = {Perceptual Multistability as Markov Chain Monte Carlo Inference},
author = {Samuel Gershman and Vul, Ed and Joshua B. Tenenbaum},
booktitle = {Advances in Neural Information Processing Systems 22},
editor = {Y. Bengio and D. Schuurmans and J. D. Lafferty and C. K. I. Williams and A. Culotta},
pages = {611--619},
year = {2009},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/3711-perceptual-multistability-as-markov-chain-monte-carlo-inference.pdf}
}







@article{Griffiths2012,
author = {Thomas L. Griffiths and Edward Vul and Adam N. Sanborn},
title ={Bridging Levels of Analysis for Probabilistic Models of Cognition},
journal = {Current Directions in Psychological Science},
volume = {21},
number = {4},
pages = {263-268},
year = {2012},
doi = {10.1177/0963721412447619},
URL = { 
        https://doi.org/10.1177/0963721412447619
},
eprint = { 
        https://doi.org/10.1177/0963721412447619
},
    abstract = { Probabilistic models of cognition characterize the abstract computational problems underlying inductive inferences and identify their ideal solutions. This approach differs from traditional methods of investigating human cognition, which focus on identifying the cognitive or neural processes that underlie behavior and therefore concern alternative levels of analysis. To evaluate the theoretical implications of probabilistic models and increase their predictive power, we must understand the relationships between theories at these different levels of analysis. One strategy for bridging levels of analysis is to explore cognitive processes that have a direct link to probabilistic inference. Recent research employing this strategy has focused on the possibility that the Monte Carlo principleâwhich concerns sampling from probability distributions in order to perform computationsâprovides a way to link probabilistic models of cognition to more concrete cognitive and neural processes. }
}

@book{Robert2005,
 author = {Robert, Christian P. and Casella, George},
 title = {Monte Carlo Statistical Methods (Springer Texts in Statistics)},
 year = {2005},
 isbn = {0387212396},
 publisher = {Springer-Verlag New York, Inc.},
 address = {Secaucus, NJ, USA},
} 

@article{Denison2013,
title = "Rational variability in children's causal inferences: The Sampling Hypothesis",
journal = "Cognition",
volume = "126",
number = "2",
pages = "285 - 300",
year = "2013",
issn = "0010-0277",
doi = "https://doi.org/10.1016/j.cognition.2012.10.010",
url = "http://www.sciencedirect.com/science/article/pii/S0010027712002491",
author = "Stephanie Denison and Elizabeth Bonawitz and Alison Gopnik and Thomas L. Griffiths",
keywords = "Cognitive development, Causal learning, Sampling Hypotheses, Probability matching, Approximate Bayesian inference",
}

@article{Sanborn2010,
          volume = {Vol.117},
          number = {No.4},
          author = {Adam N. Sanborn and Thomas L. Griffiths and Daniel J Navarro},
           title = {Rational approximations to rational models : alternative algorithms for category learning},
       publisher = {American Psychological Association},
         journal = {Psychological Review},
           pages = {1144--67},
            year = {2010},
             url = {http://wrap.warwick.ac.uk/36000/},
        abstract = {Rational models of cognition typically consider the abstract computational problems posed by the environment, assuming that people are capable of optimally solving those problems. This differs from more traditional formal models of cognition, which focus on the psychological processes responsible for behavior. A basic challenge for rational models is thus explaining how optimal solutions can be approximated by psychological processes. We outline a general strategy for answering this question, namely to explore the psychological plausibility of approximation algorithms developed in computer science and statistics. In particular, we argue that Monte Carlo methods provide a source of rational process models that connect optimal solutions to psychological processes. We support this argument through a detailed example, applying this approach to Anderson's (1990, 1991) rational model of categorization (RMC), which involves a particularly challenging computational problem. Drawing on a connection between the RMC and ideas from nonparametric Bayesian statistics, we propose 2 alternative algorithms for approximate inference in this model. The algorithms we consider include Gibbs sampling, a procedure appropriate when all stimuli are presented simultaneously, and particle filters, which sequentially approximate the posterior distribution with a small number of samples that are updated as new data become available. Applying these algorithms to several existing datasets shows that a particle filter with a single particle provides a good description of human inferences.}
}

@article{Ullman2012,
	author = {D. Ullman, Tomer and D. Goodman, Noah and B. Tenenbaum, Joshua},
	year = {2012},
	title = {Theory learning as stochastic search in the language of thought},
	journal = {Cognitive Development}
}

@article{Thaker2017,
title = "Online learning of symbolic concepts",
journal = "Journal of Mathematical Psychology",
volume = "77",
number = "Supplement C",
pages = "10 - 20",
year = "2017",
issn = "0022-2496",
doi = "https://doi.org/10.1016/j.jmp.2017.01.002",
url = "http://www.sciencedirect.com/science/article/pii/S002224961730010X",
author = "Pratiksha Thaker and Joshua B. Tenenbaum and Samuel J. Gershman",
keywords = "Bayesian models, Concept learning, Bounded rationality, Particle filtering",
abstract = "Abstract Learning complex symbolic concepts requires a rich hypothesis space, but exploring such spaces is intractable. We describe how sampling algorithms can be brought to bear on this problem, leading to the prediction that humans will exhibit the same failure modes as sampling algorithms. In particular, we show that humans get stuck in âgarden pathsââinitially promising hypotheses that turn out to be sub-optimal in light of subsequent data. Susceptibility to garden paths is sensitive to the availability of cognitive resources. These phenomena are well-explained by a Bayesian model in which humans stochastically update a sample-based representation of the posterior over a compositional hypothesis space. Our model provides a framework for understanding âbounded rationalityâ in symbolic concept learning."
}

@article{Buesing2011,
    author = {Buesing, Lars AND Bill, Johannes AND Nessler, Bernhard AND Maass, Wolfgang},
    journal = {PLOS Computational Biology},
    publisher = {Public Library of Science},
    title = {Neural Dynamics as Sampling: A Model for Stochastic Computation in Recurrent Networks of Spiking Neurons},
    year = {2011},
    month = {11},
    volume = {7},
    url = {https://doi.org/10.1371/journal.pcbi.1002211},
    pages = {1-22},
    abstract = {Author Summary It is well-known that neurons communicate with short electric pulses, called action potentials or spikes. But how can spiking networks implement complex computations? Attempts to relate spiking network activity to results of deterministic computation steps, like the output bits of a processor in a digital computer, are conflicting with findings from cognitive science and neuroscience, the latter indicating the neural spike output in identical experiments changes from trial to trial, i.e., neurons are âunreliableâ. Therefore, it has been recently proposed that neural activity should rather be regarded as samples from an underlying probability distribution over many variables which, e.g., represent a model of the external world incorporating prior knowledge, memories as well as sensory input. This hypothesis assumes that networks of stochastically spiking neurons are able to emulate powerful algorithms for reasoning in the face of uncertainty, i.e., to carry out probabilistic inference. In this work we propose a detailed neural network model that indeed fulfills these computational requirements and we relate the spiking dynamics of the network to concrete probabilistic computations. Our model suggests that neural systems are suitable to carry out probabilistic inference by using stochastic, rather than deterministic, computing elements.},
    number = {11},
    doi = {10.1371/journal.pcbi.1002211}
}

@book{Doya2007,
	author = {Doya, Kenji and Ishii, Shin and Pouget, Alexandre and Rao, Rajesh},
	year = {2007},
	month = {01},
	pages = {},
	title = {Bayesian brain. Probabilistic approaches to neural coding},
	publisher = {MIT Press, Cambridge, MA}
}

@article{MorenoBote2011,
author = {Moreno-Bote, Rubén and Knill, David C. and Pouget, Alexandre}, 
title = {Bayesian sampling in visual perception},
volume = {108}, 
number = {30}, 
pages = {12491-12496}, 
year = {2011}, 
doi = {10.1073/pnas.1101430108}, 
abstract ={It is well-established that some aspects of perception and action can be understood as probabilistic inferences over underlying probability distributions. In some situations, it would be advantageous for the nervous system to sample interpretations from a probability distribution rather than commit to a particular interpretation. In this study, we asked whether visual percepts correspond to samples from the probability distribution over image interpretations, a form of sampling that we refer to as Bayesian sampling. To test this idea, we manipulated pairs of sensory cues in a bistable display consisting of two superimposed moving drifting gratings, and we asked subjects to report their perceived changes in depth ordering. We report that the fractions of dominance of each percept follow the multiplicative rule predicted by Bayesian sampling. Furthermore, we show that attractor neural networks can sample probability distributions if input currents add linearly and encode probability distributions with probabilistic population codes.}, 
URL = {http://www.pnas.org/content/108/30/12491.abstract}, 
eprint = {http://www.pnas.org/content/108/30/12491.full.pdf}, 
journal = {Proceedings of the National Academy of Sciences} 
}

@article {Gershman2015,
    author = {Gershman, Samuel J. and Horvitz, Eric J. and Tenenbaum, Joshua B.},
    title = {Computational rationality: A converging paradigm for intelligence in brains, minds, and machines},
    volume = {349},
    number = {6245},
    pages = {273--278},
    year = {2015},
    doi = {10.1126/science.aac6076},
    publisher = {American Association for the Advancement of Science},
    abstract = {After growing up together, and mostly growing apart in the second half of the 20th century, the fields of artificial intelligence (AI), cognitive science, and neuroscience are reconverging on a shared view of the computational foundations of intelligence that promotes valuable cross-disciplinary exchanges on questions, methods, and results. We chart advances over the past several decades that address challenges of perception and action under uncertainty through the lens of computation. Advances include the development of representations and inferential procedures for large-scale probabilistic inference and machinery for enabling reflection and decisions about tradeoffs in effort, precision, and timeliness of computations. These tools are deployed toward the goal of computational rationality: identifying decisions with highest expected utility, while taking into consideration the costs of computation in complex real-world problems in which most relevant calculations can only be approximated. We highlight key concepts with examples that show the potential for interchange between computer science, cognitive science, and neuroscience.},
    issn = {0036-8075},
    URL = {http://science.sciencemag.org/content/349/6245/273},
    eprint = {http://science.sciencemag.org/content/349/6245/273.full.pdf},
    journal = {Science}
}


@Article{Smith2013b,
    author={Smith, Kyle and Graybiel, Ann},
    title={A Dual Operator View of Habitual Behavior Reflecting Cortical and Striatal Dynamics},
    journal={Neuron},
    year={2013},
    publisher={Elsevier},
    volume={79},
    number={2},
    pages={361-374},
    abstract={Habits are notoriously difficult to break and, if broken, are usually replaced by new routines. To examine the neural basis of these characteristics, we recorded spike activity in cortical and striatal habit sites as rats learned maze tasks. Overtraining induced a shift from purposeful to habitual behavior. This shift coincided with the activation of neuronal ensembles in the infralimbic neocortex and the sensorimotor striatum, which became engaged simultaneously but developed changes in spike activity with distinct time courses and stability. The striatum rapidly acquired an action-bracketing activity pattern insensitive to reward devaluation but sensitive to running automaticity. A similar pattern developed in the upper layers of the infralimbic cortex, but it formed only late during?overtraining and closely tracked habit states. Selective optogenetic disruption of infralimbic activity during overtraining prevented habit formation. We suggest that learning-related spiking dynamics of both striatum and neocortex are necessary, as dual operators, for habit crystallization.},
    issn={0896-6273},
    doi={10.1016/j.neuron.2013.05.038},
    url={http://dx.doi.org/10.1016/j.neuron.2013.05.038}
}

@article {Barnes2011,
    author = {Barnes, Terra D. and Mao, Jian-Bin and Hu, Dan and Kubota, Yasuo and Dreyer, Anna A. and Stamoulis, Catherine and Brown, Emery N. and Graybiel, Ann M.},
    title = {Advance cueing produces enhanced action-boundary patterns of spike activity in the sensorimotor striatum},
    volume = {105},
    number = {4},
    pages = {1861--1878},
    year = {2011},
    doi = {10.1152/jn.00871.2010},
    publisher = {American Physiological Society},
    abstract = {One of the most characteristic features of habitual behaviors is that they can be evoked by a single cue. In the experiments reported here, we tested for the effects of such advance cueing on the firing patterns of striatal neurons in the sensorimotor striatum. Rats ran in a T-maze with instruction cues about the location of reward given at the start of the runs. This advance cueing about reward produced a highly augmented task-bracketing pattern of activity at the beginning and end of procedural task performance relative to the patterns found previously with midtask cueing. Remarkably, the largest increase in activity early during the T-maze runs was not associated with the instruction cues themselves, the earliest predictors of reward; instead, the highest peak of early activity was associated with the beginning of the motor period of the task. We suggest that the advance cueing, reducing midrun demands for decision making but adding a working-memory load, facilitated chunking of the maze runs as executable scripts anchored to sensorimotor aspects of the task and unencumbered by midtask decision-making demands. Our findings suggest that the acquisition of stronger task-bracketing patterns of striatal activity in the sensorimotor striatum could reflect this enhancement of behavioral chunking. Deficits in such representations of learned sequential behaviors could contribute to motor and cognitive problems in a range of neurological disorders affecting the basal ganglia, including Parkinson{\textquoteright}s disease.},
    issn = {0022-3077},
    URL = {http://jn.physiology.org/content/105/4/1861},
    eprint = {http://jn.physiology.org/content/105/4/1861.full.pdf},
    journal = {Journal of Neurophysiology}
}

@Article{Jin2014,
	author={Jin, Xin
	and Tecuapetla, Fatuel
	and Costa, Rui M.},
	title={Basal ganglia subcircuits distinctively encode the parsing and concatenation of action sequences},
	journal={Nature Neuroscience},
	year={2014},
	month={Jan},
	day={26},
	publisher={Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved. SN  -},
	volume={17},
	pages={423 EP  -},
	note={Article},
	url={http://dx.doi.org/10.1038/nn.3632}
}

@article {Ainge2007,
    author = {Ainge, James A. and Tamosiunaite, Minija and Woergoetter, Florentin and Dudchenko, Paul A.},
    title = {Hippocampal CA1 Place Cells Encode Intended Destination on a Maze with Multiple Choice Points},
    volume = {27},
    number = {36},
    pages = {9769--9779},
    year = {2007},
    doi = {10.1523/JNEUROSCI.2011-07.2007},
    publisher = {Society for Neuroscience},
    abstract = {The hippocampus encodes both spatial and nonspatial aspects of a rat{\textquoteright}s ongoing behavior at the single-cell level. In this study, we examined the encoding of intended destination by hippocampal (CA1) place cells during performance of a serial reversal task on a double Y-maze. On the maze, rats had to make two choices to access one of four possible goal locations, two of which contained reward. Reward locations were kept constant within blocks of 10 trials but changed between blocks, and the session of each day comprised three or more trial blocks. A disproportionate number of place fields were observed in the start box and beginning stem of the maze, relative to other locations on the maze. Forty-six percent of these place fields had different firing rates on journeys to different goal boxes. Another group of cells had place fields before the second choice point, and, of these, 44\% differentiated between journeys to specific goal boxes. In a second experiment, we observed that rats with hippocampal damage made significantly more errors than control rats on the Y-maze when reward locations were reversed. Together, these results suggest that, at the start of the maze, the hippocampus encodes both current location and the intended destination of the rat, and this encoding is necessary for the flexible response to changes in reinforcement contingencies.},
    issn = {0270-6474},
    URL = {http://www.jneurosci.org/content/27/36/9769},
    eprint = {http://www.jneurosci.org/content/27/36/9769.full.pdf},
    journal = {Journal of Neuroscience}
}

@Article{Jin2010,
author={Jin, Xin
and Costa, Rui M.},
title={Start/stop signals emerge in nigrostriatal circuits during sequence learning},
journal={Nature},
year={2010},
month={Jul},
day={22},
publisher={Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
volume={466},
number={7305},
pages={457-462},
issn={0028-0836},
doi={10.1038/nature09263},
url={http://dx.doi.org/10.1038/nature09263}
}

@Article{Eichenbaum2017,
author={Eichenbaum, Howard},
title={On the Integration of Space, Time, and Memory},
journal={Neuron},
year={2017},
month={2017/10/31},
publisher={Elsevier},
volume={95},
number={5},
pages={1007-1018},
abstract={The hippocampus is famous for mapping locations in spatially organized environments, and several recent studies have shown that hippocampal networks also map moments in temporally organized experiences. Here I consider how space and time are integrated in the representation of memories. The brain pathways for spatial and temporal cognition involve overlapping and interacting systems that converge on the hippocampal region. There is evidence that spatial and temporal aspects of memory are processed somewhat differently in the circuitry of hippocampal subregions but become fully integrated within CA1 neuronal networks as independent, multiplexed representations of space and time. Hippocampal networks also map memories across a broad range of abstract relations among events, suggesting that the findings on spatial and temporal organization reflect a generalized mechanism for organizing memories.},
issn={0896-6273},
doi={10.1016/j.neuron.2017.06.036},
url={http://dx.doi.org/10.1016/j.neuron.2017.06.036}
}

@Article{Wood2000,
author={Wood, Emma R.
and Dudchenko, Paul A.
and Robitsek, R. Jonathan
and Eichenbaum, Howard},
title={Hippocampal Neurons Encode Information about Different Types of Memory Episodes Occurring in the Same Location},
journal={Neuron},
year={2000},
month={2017/10/31},
publisher={Elsevier},
volume={27},
number={3},
pages={623-633},
abstract={Firing patterns of hippocampal complex-spike neurons were examined for the capacity to encode information important to the memory demands of a task even when the overt behavior and location of the animal are held constant. Neuronal activity was recorded as rats continuously alternated left and right turns from the central stem of a modified T maze. Two-thirds of the cells fired differentially as the rat traversed the common stem on left-turn and right-turn trials, even when potentially confounding variations in running speed, heading, and position on the stem were taken into account. Other cells fired differentially on the two trial types in combination with behavioral and spatial factors or appeared to fire similarly on both trial types. This pattern of results suggests that hippocampal representations encode some of the information necessary for representing specific memory episodes.},
issn={0896-6273},
doi={10.1016/S0896-6273(00)00071-4},
url={http://dx.doi.org/10.1016/S0896-6273(00)00071-4}
}

@article {Ferbinteanu2011,
    author = {Ferbinteanu, Janina and Shirvalkar, Prasad and Shapiro, Matthew L.},
    title = {Memory Modulates Journey-Dependent Coding in the Rat Hippocampus},
    volume = {31},
    number = {25},
    pages = {9135--9146},
    year = {2011},
    doi = {10.1523/JNEUROSCI.1241-11.2011},
    publisher = {Society for Neuroscience},
    abstract = {Neurons in the rat hippocampus signal current location by firing in restricted areas called place fields. During goal-directed tasks in mazes, place fields can also encode past and future positions through journey-dependent activity, which could guide hippocampus-dependent behavior and underlie other temporally extended memories, such as autobiographical recollections. The relevance of journey-dependent activity for hippocampal-dependent memory, however, is not well understood. To further investigate the relationship between hippocampal journey-dependent activity and memory, we compared neural firing in rats performing two mnemonically distinct but behaviorally identical tasks in the plus maze: a hippocampus-dependent spatial navigation task and a hippocampus-independent cue response task. While place, prospective, and retrospective coding reflected temporally extended behavioral episodes in both tasks, memory strategy altered coding differently before and after the choice point. Before the choice point, when discriminative selection of memory strategy was critical, a switch between the tasks elicited a change in a field{\textquoteright}s coding category, so that a field that signaled current location in one task coded pending journeys in the other task. After the choice point, however, when memory strategy became irrelevant, the fields preserved coding categories across tasks, so that the same field consistently signaled either current location or the recent journeys. Additionally, on the start arm, firing rates were affected at comparable levels by task and journey; on the goal arm, firing rates predominantly encoded journey. The data demonstrate a direct link between journey-dependent coding and memory and suggest that episodes are encoded by both population and firing rate coding.},
    issn = {0270-6474},
    URL = {http://www.jneurosci.org/content/31/25/9135},
    eprint = {http://www.jneurosci.org/content/31/25/9135.full.pdf},
    journal = {Journal of Neuroscience}
}

@Article{Stachenfeld2017,
author={Stachenfeld, Kimberly L.
and Botvinick, Matthew M.
and Gershman, Samuel J.},
title={The hippocampus as a predictive map},
journal={Nat Neurosci},
year={2017},
month={Nov},
publisher={Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
volume={20},
number={11},
pages={1643-1653},
abstract={A cognitive map has long been the dominant metaphor for hippocampal function, embracing the idea that place cells encode a geometric representation of space. However, evidence for predictive coding, reward sensitivity and policy dependence in place cells suggests that the representation is not purely spatial. We approach this puzzle from a reinforcement learning perspective: what kind of spatial representation is most useful for maximizing future reward? We show that the answer takes the form of a predictive representation. This representation captures many aspects of place cell responses that fall outside the traditional view of a cognitive map. Furthermore, we argue that entorhinal grid cells encode a low-dimensionality basis set for the predictive representation, useful for suppressing noise in predictions and extracting multiscale structure for hierarchical planning.},
note={Article},
issn={1097-6256},
url={http://dx.doi.org/10.1038/nn.4650}
}

@book{Bellman1957,
 author = {Bellman, Richard},
 title = {Dynamic Programming},
 year = {1957},
 publisher = {Princeton University Press},
 address = {Princeton, NJ, USA},
} 

@incollection{DoshiVelez2009,
	title = {The Infinite Partially Observable Markov Decision Process},
	author = {Finale Doshi-Velez},
	booktitle = {Advances in Neural Information Processing Systems 22},
	editor = {Y. Bengio and D. Schuurmans and J. D. Lafferty and C. K. I. Williams and A. Culotta},
	pages = {477--485},
	year = {2009},
	publisher = {Curran Associates, Inc.},
	url = {http://papers.nips.cc/paper/3780-the-infinite-partially-observable-markov-decision-process.pdf}
}

@phdthesis{Duff2002,
 author = {Duff, Michael O'Gordon},
 title = {Optimal Learning: Computational Procedures for Bayes-adaptive Markov Decision Processes},
 year = {2002},
 isbn = {0-493-52573-4},
 note = {AAI3039353},
 publisher = {University of Massachusetts Amherst},
} 


@article{Teh2006,
	author = {Yee Whye Teh and Michael I Jordan and Matthew J Beal and David M Blei},
	title = {Hierarchical Dirichlet Processes},
	journal = {Journal of the American Statistical Association},
	volume = {101},
	number = {476},
	pages = {1566-1581},
	year  = {2006},
	publisher = {Taylor \& Francis},
	doi = {10.1198/016214506000000302}
}

@InProceedings{Stepleton2009,
  title =      {The Block Diagonal Infinite Hidden Markov Model},
  author =      {Thomas Stepleton and Zoubin Ghahramani and Geoffrey Gordon and Tai-Sing Lee},
  booktitle =      {Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics},
  pages =      {552--559},
  year =      {2009},
  editor =      {David van Dyk and Max Welling},
  volume =      {5},
  series =      {Proceedings of Machine Learning Research},
  address =      {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},
  month =      {16--18 Apr},
  publisher =      {PMLR},
  pdf =      {http://proceedings.mlr.press/v5/stepleton09a/stepleton09a.pdf},
  url =      {http://proceedings.mlr.press/v5/stepleton09a.html},
  abstract =      {The Infinite Hidden Markov Model (IHMM) extends hidden Markov models to have a countably infinite number of hidden states \citeihmm,hdp. We present a generalization of this framework that introduces block-diagonal structure in the transitions between the hidden states. These blocks correspond to “sub-behaviors” exhibited by data sequences. In identifying such structure, the model classifies, or partitions, sequence data according to these sub-behaviors in an unsupervised way. We present an application of this model to artificial data, a video gesture classification task, and a musical theme labeling task, and show that components of the model can also be applied to graph segmentation.}
}

@inproceedings{Dietterich1998,
 author = {Dietterich, Thomas G.},
 title = {The MAXQ Method for Hierarchical Reinforcement Learning},
 booktitle = {Proceedings of the Fifteenth International Conference on Machine Learning},
 series = {ICML '98},
 year = {1998},
 isbn = {1-55860-556-8},
 pages = {118--126},
 numpages = {9},
 url = {http://dl.acm.org/citation.cfm?id=645527.657449},
 acmid = {657449},
 publisher = {Morgan Kaufmann Publishers Inc.},
 address = {San Francisco, CA, USA},
} 


@article{Dietterich2000,
 author = {Dietterich, Thomas G.},
 title = {Hierarchical Reinforcement Learning with the MAXQ Value Function Decomposition},
 journal = {J. Artif. Int. Res.},
 issue_date = {August 2000},
 volume = {13},
 number = {1},
 month = nov,
 year = {2000},
 issn = {1076-9757},
 pages = {227--303},
 numpages = {77},
 url = {http://dl.acm.org/citation.cfm?id=1622262.1622268},
 acmid = {1622268},
 publisher = {AI Access Foundation},
 address = {USA},
} 










@article{balaguer2016neural,
  title={Neural mechanisms of hierarchical planning in a virtual subway network},
  author={Balaguer, Jan and Spiers, Hugo and Hassabis, Demis and Summerfield, Christopher},
  journal={Neuron},
  volume={90},
  number={4},
  pages={893--903},
  year={2016},
  publisher={Elsevier}
}

@book{fernandez2013multi,
  title={Multi-hierarchical representation of large-scale space: Applications to mobile robots},
  author={Fern{\'a}ndez, Juan A and Gonz{\'a}lez, Javier},
  volume={24},
  year={2013},
  publisher={Springer Science \& Business Media}
}


@article{miller1956magic,
  title={The magic number seven plus or minus two: Some limits on our capacity for processing information},
  author={Miller, George A},
  journal={Psychological review},
  volume={63},
  pages={91--97},
  year={1956}
}

@article{gershman2012tutorial,
  title={A tutorial on Bayesian nonparametric models},
  author={Gershman, Samuel J and Blei, David M},
  journal={Journal of Mathematical Psychology},
  volume={56},
  number={1},
  pages={1--12},
  year={2012},
  publisher={Elsevier}
}

@misc{casella1999monte,
  title={Monte Carlo Statistical Methods},
  author={Casella, G},
  year={1999},
  publisher={Springer-Verlag, New York}
}

@article{roberts2009examples,
  title={Examples of adaptive MCMC},
  author={Roberts, Gareth O and Rosenthal, Jeffrey S},
  journal={Journal of Computational and Graphical Statistics},
  volume={18},
  number={2},
  pages={349--367},
  year={2009},
  publisher={Taylor \& Francis}
}

@article{neal2000markov,
  title={Markov chain sampling methods for Dirichlet process mixture models},
  author={Neal, Radford M},
  journal={Journal of computational and graphical statistics},
  volume={9},
  number={2},
  pages={249--265},
  year={2000},
  publisher={Taylor \& Francis}
}

@book{sutton2018reinforcement,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S and Barto, Andrew G},
  year={2018},
  publisher={MIT press}
}

@article{robert2004casella,
  title={Casella: Monte Carlo Statistical Methods},
  author={Robert, Christian P},
  journal={Springerverlag, New York},
  volume={3},
  year={2004}
}

@incollection{doucet2001introduction,
  title={An introduction to sequential Monte Carlo methods},
  author={Doucet, Arnaud and De Freitas, Nando and Gordon, Neil},
  booktitle={Sequential Monte Carlo methods in practice},
  pages={3--14},
  year={2001},
  publisher={Springer}
}

@article{thaker2017online,
  title={Online learning of symbolic concepts},
  author={Thaker, Pratiksha and Tenenbaum, Joshua B and Gershman, Samuel J},
  journal={Journal of Mathematical Psychology},
  volume={77},
  pages={10--20},
  year={2017},
  publisher={Elsevier}
}


@article{chopin2002sequential,
  title={A sequential particle filter method for static models},
  author={Chopin, Nicolas},
  journal={Biometrika},
  volume={89},
  number={3},
  pages={539--552},
  year={2002},
  publisher={Oxford University Press}
}


@article{murphy2001active,
  title={Active learning of causal Bayes net structure},
  author={Murphy, Kevin P},
  year={2001},
  publisher={Citeseer}
}

@inproceedings{tong2001active,
  title={Active learning for structure in Bayesian networks},
  author={Tong, Simon and Koller, Daphne},
  booktitle={International joint conference on artificial intelligence},
  volume={17},
  number={1},
  pages={863--869},
  year={2001},
  organization={Citeseer}
}

@article{steyvers2003inferring,
  title={Inferring causal networks from observations and interventions},
  author={Steyvers, Mark and Tenenbaum, Joshua B and Wagenmakers, Eric-Jan and Blum, Ben},
  journal={Cognitive science},
  volume={27},
  number={3},
  pages={453--489},
  year={2003},
  publisher={Wiley Online Library}
}

@article{nair2006entropy,
  title={On entropy for mixtures of discrete and continuous variables},
  author={Nair, Chandra and Prabhakar, Balaji and Shah, Devavrat},
  journal={arXiv preprint cs/0607075},
  year={2006}
}





% -----------------------------------------------------------------------
% Document End
 -----------------------------------------------------------------------
